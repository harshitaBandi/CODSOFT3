{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3MIMnRwCRkEE"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "# Load the pre-trained image recognition model\n",
        "vgg16 = tf.keras.applications.VGG16(weights='imagenet')\n",
        "\n",
        "# Define the RNN model\n",
        "class RNNCaptioningModel(tf.keras.Model):\n",
        "    def __init__(self, vocab_size):\n",
        "        super(RNNCaptioningModel, self).__init__()\n",
        "\n",
        "        # Create an embedding layer to convert words to vectors\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, 128)\n",
        "\n",
        "        # Create an RNN layer to generate the caption\n",
        "        self.rnn = tf.keras.layers.LSTM(128)\n",
        "\n",
        "        # Create a fully connected layer to predict the next word in the sequence\n",
        "        self.output = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "    def call(self, image_features):\n",
        "        # Embed the image features\n",
        "        embedded_image_features = self.embedding(image_features)\n",
        "\n",
        "        # Initialize the RNN state\n",
        "        rnn_state = None\n",
        "\n",
        "        # Generate the caption one word at a time\n",
        "        caption = []\n",
        "        for i in range(100):\n",
        "            # Pass the current image features and RNN state to the RNN model\n",
        "            output, rnn_state = self.rnn(embedded_image_features, rnn_state)\n",
        "\n",
        "            # Predict the next word in the sequence\n",
        "            next_word = tf.argmax(self.output(output), axis=1)\n",
        "\n",
        "            # Add the predicted word to the caption\n",
        "            caption.append(next_word)\n",
        "\n",
        "            # Stop generating the caption if we reach the end-of-sequence token\n",
        "            if next_word == tf.constant(0):\n",
        "                break\n",
        "\n",
        "        return caption\n",
        "\n",
        "# Load the image captioning dataset\n",
        "(train_images, train_captions), (val_images, val_captions) = tf.keras.datasets.flickr8k.load_data()\n",
        "\n",
        "# Create a vocabulary from the training captions\n",
        "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=5000)\n",
        "tokenizer.fit_on_texts(train_captions)\n",
        "\n",
        "# Convert the training and validation captions to word vectors\n",
        "train_captions_vectors = tokenizer.texts_to_sequences(train_captions)\n",
        "val_captions_vectors = tokenizer.texts_to_sequences(val_captions)\n",
        "\n",
        "# Pad the captions to the same length\n",
        "max_caption_length = 100\n",
        "train_captions_vectors = tf.keras.preprocessing.sequence.pad_sequences(train_captions_vectors, maxlen=max_caption_length)\n",
        "val_captions_vectors = tf.keras.preprocessing.sequence.pad_sequences(val_captions_vectors, maxlen=max_caption_length)\n",
        "\n",
        "# Create the image captioning model\n",
        "model = RNNCaptioningModel(5000)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(train_images, train_captions_vectors, epochs=10)\n",
        "\n",
        "# Evaluate the model\n",
        "val_loss, val_accuracy = model.evaluate(val_images, val_captions_vectors)\n",
        "\n",
        "print('Validation loss:', val_loss)\n",
        "print('Validation accuracy:', val_accuracy)\n",
        "\n",
        "# Generate a caption for a new image\n",
        "new_image = tf.image.decode_jpeg(tf.io.read_file('image.jpg'))\n",
        "new_image_features = vgg16.predict(new_image[tf.newaxis, ...])\n",
        "\n",
        "# Generate the caption\n",
        "caption = model(new_image_features)[0]\n",
        "\n",
        "# Decode the caption\n",
        "decoded_caption = tokenizer.sequences_to_texts([caption])[0]\n",
        "\n",
        "# Print the caption\n",
        "print('Caption:', decoded_caption)\n"
      ]
    }
  ]
}